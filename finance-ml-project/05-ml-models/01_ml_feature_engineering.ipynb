{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce80994-ee92-45a4-b0a4-53324949ccdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Pipeline Machine Learning 3 modelos \n",
    "# 1. Random Forest - Classificação de Categorias\n",
    "# 2. Isolation Forest - Detecção de Anomalias\n",
    "# 3. K-Means - Clustering de Padrões\n",
    "# ============================================\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" \" * 20 + \"Pipeline Analise Financeira\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47182feb-89d3-4c58-83de-85587dc54f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Etapa 1: Configurar Mlflow\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Etapa 1 Mlflow\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "experiment_path = \"/Users/andre.bomfim99@gmail.com/finance-ml-experiments\"\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(experiment_path)\n",
    "    print(f\" Experimento criado: {experiment_path}\")\n",
    "    print(f\"   ID: {experiment_id}\")\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"  Experimento já existe: {experiment_path}\")\n",
    "    else:\n",
    "        print(f\"  Erro ao criar experimento: {e}\")\n",
    "        experiment_path = \"finance-ml-project\"\n",
    "        print(f\"   Usando nome simples: {experiment_path}\")\n",
    "\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\" MLflow configurado ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7343879d-52b9-4e8c-b52f-fd400107fc7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Etapa2: carrega os dados\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Etapa 2: carrega os dados\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "SOURCE_TABLE = \"finance_silver.transacoes_silver\"\n",
    "print(f\"\\n Carregando: {SOURCE_TABLE}\")\n",
    "\n",
    "df = spark.read.table(SOURCE_TABLE)\n",
    "total_records = df.count()\n",
    "\n",
    "print(f\" {total_records:,} transações carregadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd5523a6-fe27-4477-817b-bd2262a6a580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Etapa 3: Feature eng\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Etapa 3: Feature eng\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_pandas = df.select(\n",
    "    \"data\", \"descricao\", \"valor\", \"tipo\", \"categoria\",  \n",
    "    \"ano\", \"mes\", \"dia_semana\", \"trimestre\"\n",
    ").toPandas()\n",
    "\n",
    "print(f\"\\n Dataframe Pandas criado: {len(df_pandas):,} linhas\")\n",
    "df_pandas['descricao_length'] = df_pandas['descricao'].str.len()\n",
    "keywords = {\n",
    "    'salario': ['SALARIO', 'PAGAMENTO', 'FUNCIONARIO'],\n",
    "    'fornecedor': ['FORNECEDOR', 'TED', 'COMPRA'],\n",
    "    'receita': ['RECEBIDO', 'CLIENTE', 'VENDA'],\n",
    "    'imposto': ['INSS', 'IR', 'ISS', 'IMPOSTO']\n",
    "}\n",
    "\n",
    "for key, words in keywords.items():\n",
    "    df_pandas[f'keyword_{key}'] = df_pandas['descricao'].apply(\n",
    "        lambda x: 1 if any(word in str(x) for word in words) else 0\n",
    "    )\n",
    "\n",
    "df_pandas['tipo_num'] = df_pandas['tipo'].map({'entrada': 1, 'saida': 0})\n",
    "df_pandas['valor_log'] = np.log1p(df_pandas['valor'])\n",
    "df_pandas['dia'] = pd.to_datetime(df_pandas['data']).dt.day\n",
    "df_pandas['fim_mes'] = (df_pandas['dia'] > 25).astype(int)\n",
    "print(\"\\n Features criadas:\")\n",
    "features_created = [\n",
    "    'descricao_length', 'keyword_salario', 'keyword_fornecedor',\n",
    "    'keyword_receita', 'keyword_imposto', 'tipo_num', 'valor_log', 'fim_mes'\n",
    "]\n",
    "for i, feat in enumerate(features_created, 1):\n",
    "    print(f\"   {i}. {feat}\")\n",
    "df_classification = df_pandas.copy()\n",
    "print(f\"\\n Dataset preparado: {len(df_classification):,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d521856f-3568-4708-9dbe-331741f2c5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Etapa 4 Modelo 1 Random Forest\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Etapa 4 Modelo 1 Random Forest \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "features = [\n",
    "    'valor', 'valor_log', 'tipo_num', 'dia_semana', 'mes', 'trimestre',\n",
    "    'descricao_length', 'keyword_salario', 'keyword_fornecedor', \n",
    "    'keyword_receita', 'keyword_imposto', 'fim_mes'\n",
    "]\n",
    "\n",
    "X = df_classification[features]\n",
    "y = df_classification['categoria']\n",
    "\n",
    "print(f\"\\n Informações do Dataset:\")\n",
    "print(f\"   Features: {len(features)}\")\n",
    "print(f\"   Samples: {len(X):,}\")\n",
    "print(f\"   Classes: {y.nunique()}\")\n",
    "\n",
    "print(f\"\\n Distribuição de Classes:\")\n",
    "class_dist = y.value_counts()\n",
    "for categoria, count in class_dist.items():\n",
    "    print(f\"   {categoria:20s}: {count:6,} ({count/len(y)*100:5.1f}%)\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n  Split de Dados:\")\n",
    "print(f\"   Train: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test:  {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"1_RandomForest_Classification\") as run:\n",
    "    \n",
    "    print(\"\\n Treinando Random Forest\")\n",
    "    \n",
    "    model_rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model_rf.fit(X_train, y_train)\n",
    "    y_pred = model_rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n modelo treinado\")\n",
    "    print(f\"   Acurácia: {accuracy:.2%}\")\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 200)\n",
    "    mlflow.log_param(\"max_depth\", 15)\n",
    "    mlflow.log_param(\"class_weight\", \"balanced\")\n",
    "    mlflow.log_param(\"features_count\", len(features))\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    for categoria in y.unique():\n",
    "        if categoria in report_dict:\n",
    "            mlflow.log_metric(f\"precision_{categoria}\", report_dict[categoria]['precision'])\n",
    "            mlflow.log_metric(f\"recall_{categoria}\", report_dict[categoria]['recall'])\n",
    "            mlflow.log_metric(f\"f1_{categoria}\", report_dict[categoria]['f1-score'])\n",
    "    \n",
    "    mlflow.sklearn.log_model(model_rf, \"random_forest_model\", input_example=X_train.head(1))\n",
    "    print(f\"   MLflow Run ID: {run.info.run_id}\")\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=model_rf.classes_, yticklabels=model_rf.classes_,\n",
    "                cbar_kws={'label': 'Quantidade'})\n",
    "    plt.title('Matriz de Confusão Random Forest', fontsize=16, pad=20)\n",
    "    plt.ylabel('Categoria Real', fontsize=12)\n",
    "    plt.xlabel('Categoria Predita', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/rf_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact('/tmp/rf_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model_rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n Top 5 Features Mais Importantes:\")\n",
    "    print(feature_importance.head(5).to_string(index=False))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importância', fontsize=12)\n",
    "    plt.title('Top 10 Features - Random Forest', fontsize=14, pad=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/rf_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact('/tmp/rf_feature_importance.png')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n Modelo 1 Random Forest concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c1a7e9-3e6e-44b4-a3bf-88af4a1d27ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================\n",
    "# Etapa 5 Modelo 2 isolation forest\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Etapa 5 Modelo 2 isolation forest detectando anomalias\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "anomaly_features = ['valor', 'dia_semana', 'mes']\n",
    "X_anomaly = df_classification[anomaly_features].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_anomaly_scaled = scaler.fit_transform(X_anomaly)\n",
    "\n",
    "print(f\"\\n Informações:\")\n",
    "print(f\"   Features: {anomaly_features}\")\n",
    "print(f\"   Samples: {len(X_anomaly):,}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"2_IsolationForest_Anomaly\") as run:\n",
    "    \n",
    "    print(\"\\n Treinando Isolation Forest\")\n",
    "    \n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=0.05,\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    predictions = iso_forest.fit_predict(X_anomaly_scaled)\n",
    "    scores = iso_forest.score_samples(X_anomaly_scaled)\n",
    "    \n",
    "    df_classification['anomalia'] = predictions\n",
    "    df_classification['anomaly_score'] = scores\n",
    "    \n",
    "    num_anomalias = (predictions == -1).sum()\n",
    "    pct_anomalias = (num_anomalias / len(predictions)) * 100\n",
    "    \n",
    "    print(f\"\\n Modelo treinado\")\n",
    "    print(f\"   Anomalias detectadas: {num_anomalias:,} ({pct_anomalias:.1f}%)\")\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"IsolationForest\")\n",
    "    mlflow.log_param(\"contamination\", 0.05)\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"features\", str(anomaly_features))\n",
    "    mlflow.log_metric(\"anomalias_detectadas\", num_anomalias)\n",
    "    mlflow.log_metric(\"pct_anomalias\", pct_anomalias)\n",
    "    mlflow.sklearn.log_model(iso_forest, \"isolation_forest_model\")\n",
    "    mlflow.sklearn.log_model(scaler, \"scaler\")\n",
    "    \n",
    "    print(f\"   MLflow Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    print(\"\\n 10 Transações Mais Anômalas:\")\n",
    "    anomalias_df = df_classification[df_classification['anomalia'] == -1].copy()\n",
    "    anomalias_df = anomalias_df.sort_values('anomaly_score')\n",
    "    \n",
    "    display(anomalias_df[[\n",
    "        'data', 'descricao', 'valor', 'categoria', 'anomaly_score'\n",
    "    ]].head(10))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    axes[0].hist(scores, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0].axvline(x=scores[predictions == -1].max(), color='red', \n",
    "                    linestyle='--', linewidth=2, label='Threshold Anomalia')\n",
    "    axes[0].set_xlabel('Anomaly Score', fontsize=11)\n",
    "    axes[0].set_ylabel('Frequência', fontsize=11)\n",
    "    axes[0].set_title('Distribuição dos Anomaly Scores', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    anomaly_by_category = df_classification.groupby('categoria')['anomalia'].apply(\n",
    "        lambda x: (x == -1).sum()\n",
    "    ).sort_values(ascending=False)\n",
    "    anomaly_by_category.plot(kind='bar', ax=axes[1], color='coral', edgecolor='black')\n",
    "    axes[1].set_xlabel('Categoria', fontsize=11)\n",
    "    axes[1].set_ylabel('Número de Anomalias', fontsize=11)\n",
    "    axes[1].set_title('Anomalias por Categoria', fontsize=13, fontweight='bold')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/anomaly_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact('/tmp/anomaly_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n Isolation Forest concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8ecede5-46e7-480a-9d6b-b5a6d8c6357b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================\n",
    "# Etapa 6 Modelo 3 Kmeans\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" ETAPA 6/6: K-MEANS - CLUSTERING DE PADRÕES\")\n",
    "print(\"=\" * 80)\n",
    "df_category_agg = df_classification.groupby('categoria').agg({\n",
    "    'valor': ['mean', 'sum', 'count', 'std'],\n",
    "    'tipo_num': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_category_agg.columns = ['_'.join(col).strip('_') for col in df_category_agg.columns.values]\n",
    "df_category_agg.columns = ['categoria'] + list(df_category_agg.columns[1:])\n",
    "\n",
    "print(f\"\\n Categorias para clustering: {len(df_category_agg)}\")\n",
    "cluster_features = ['valor_mean', 'valor_sum', 'valor_count', 'valor_std']\n",
    "X_cluster = df_category_agg[cluster_features].fillna(0)\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "\n",
    "print(f\"   Features: {cluster_features}\")\n",
    "print(f\"   Dados normalizados: {X_cluster_scaled.shape}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"3_KMeans_Clustering\") as run:\n",
    "    \n",
    "    print(\"\\n Treinando K-Means...\")\n",
    "    \n",
    "    kmeans = KMeans(\n",
    "        n_clusters=3, \n",
    "        random_state=42, \n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    clusters = kmeans.fit_predict(X_cluster_scaled)\n",
    "    \n",
    "    df_category_agg['cluster'] = clusters\n",
    "    \n",
    "    silhouette = silhouette_score(X_cluster_scaled, clusters)\n",
    "    \n",
    "    print(f\"\\n Modelo treinado!\")\n",
    "    print(f\"   Clusters criados: 3\")\n",
    "    print(f\"   Inertia: {kmeans.inertia_:.2f}\")\n",
    "    print(f\"   Silhouette Score: {silhouette:.3f}\")\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"KMeans\")\n",
    "    mlflow.log_param(\"n_clusters\", 3)\n",
    "    mlflow.log_param(\"features\", str(cluster_features))\n",
    "    mlflow.log_metric(\"inertia\", kmeans.inertia_)\n",
    "    mlflow.log_metric(\"silhouette_score\", silhouette)\n",
    "    mlflow.sklearn.log_model(kmeans, \"kmeans_model\")\n",
    "    mlflow.sklearn.log_model(scaler_cluster, \"scaler_cluster\")\n",
    "    \n",
    "    print(f\"   MLflow Run ID: {run.info.run_id}\")\n",
    "    print(\"\\n Clusters Identificados:\")\n",
    "    for cluster_id in sorted(df_category_agg['cluster'].unique()):\n",
    "        cluster_data = df_category_agg[df_category_agg['cluster'] == cluster_id]\n",
    "        categorias = cluster_data['categoria'].tolist()\n",
    "        \n",
    "        print(f\"\\n   Cluster {cluster_id}: {', '.join(categorias)}\")\n",
    "        print(f\"      Valor médio: R$ {cluster_data['valor_mean'].mean():,.2f}\")\n",
    "        print(f\"      Transações: {cluster_data['valor_count'].sum():.0f}\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    scatter1 = ax1.scatter(\n",
    "        df_category_agg['valor_mean'], \n",
    "        df_category_agg['valor_sum'],\n",
    "        c=df_category_agg['cluster'], \n",
    "        cmap='viridis', \n",
    "        s=200, \n",
    "        alpha=0.7,\n",
    "        edgecolors='black',\n",
    "        linewidth=2\n",
    "    )\n",
    "    for idx, row in df_category_agg.iterrows():\n",
    "        ax1.annotate(row['categoria'], (row['valor_mean'], row['valor_sum']),\n",
    "                    fontsize=9, ha='center', fontweight='bold')\n",
    "    ax1.set_xlabel('Valor Médio (R$)', fontsize=11)\n",
    "    ax1.set_ylabel('Volume Total (R$)', fontsize=11)\n",
    "    ax1.set_title('Clusters: Valor Médio vs Volume Total', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(scatter1, ax=ax1, label='Cluster')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    scatter2 = ax2.scatter(\n",
    "        df_category_agg['valor_count'], \n",
    "        df_category_agg['valor_std'],\n",
    "        c=df_category_agg['cluster'], \n",
    "        cmap='viridis', \n",
    "        s=200, \n",
    "        alpha=0.7,\n",
    "        edgecolors='black',\n",
    "        linewidth=2\n",
    "    )\n",
    "    for idx, row in df_category_agg.iterrows():\n",
    "        ax2.annotate(row['categoria'], (row['valor_count'], row['valor_std']),\n",
    "                    fontsize=9, ha='center', fontweight='bold')\n",
    "    ax2.set_xlabel('Quantidade de Transações', fontsize=11)\n",
    "    ax2.set_ylabel('Desvio Padrão (R$)', fontsize=11)\n",
    "    ax2.set_title('Clusters: Quantidade vs Variabilidade', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(scatter2, ax=ax2, label='Cluster')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    cluster_counts = df_category_agg['cluster'].value_counts().sort_index()\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(cluster_counts)))\n",
    "    ax3.bar(cluster_counts.index, cluster_counts.values, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax3.set_xlabel('Cluster', fontsize=11)\n",
    "    ax3.set_ylabel('Número de Categorias', fontsize=11)\n",
    "    ax3.set_title('Distribuição por Cluster', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(cluster_counts.index)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    cluster_profiles = df_category_agg.groupby('cluster')[cluster_features].mean()\n",
    "    sns.heatmap(cluster_profiles.T, annot=True, fmt='.0f', cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'Valor'}, ax=ax4, linewidths=1, linecolor='black')\n",
    "    ax4.set_title('Perfil Médio dos Clusters', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlabel('Cluster', fontsize=11)\n",
    "    ax4.set_ylabel('Features', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/kmeans_clusters.png', dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact('/tmp/kmeans_clusters.png')\n",
    "    plt.show()\n",
    "    \n",
    "    df_category_agg.to_csv('/tmp/clusters_resultado.csv', index=False)\n",
    "    mlflow.log_artifact('/tmp/clusters_resultado.csv')\n",
    "\n",
    "print(\"\\n Modelo K-Means concluído\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad5e855-54c5-4627-882b-02f19b1d63fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Finalizando\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" \" * 25 + \" Pipeline concluido e ok \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n Resumo Executivo:\")\n",
    "print(f\"   {'─' * 76}\")\n",
    "print(f\"   Total de transações processadas: {len(df_classification):,}\")\n",
    "print(f\"   {'─' * 76}\")\n",
    "print(f\"\\n    Modelo 1 random forest classificação\")\n",
    "print(f\"      Acurácia: {accuracy:.2%}\")\n",
    "print(f\"      Classes: {y.nunique()}\")\n",
    "print(f\"      Macro F1 Score: {report_dict['macro avg']['f1-score']:.2%}\")\n",
    "print(f\"      Weighted F1 Score: {report_dict['weighted avg']['f1-score']:.2%}\")\n",
    "print(f\"   {'─' * 76}\")\n",
    "print(f\"\\n    Modelo 2 isolation forest anomalias\")\n",
    "print(f\"      Anomalias detectadas: {num_anomalias:,} ({pct_anomalias:.1f}%)\")\n",
    "print(f\"      Transações normais: {len(df_classification) - num_anomalias:,}\")\n",
    "print(f\"   {'─' * 76}\")\n",
    "print(f\"\\n    Modelo 3 kmeans ou clustering\")\n",
    "print(f\"      Clusters criados: 3\")\n",
    "print(f\"      Silhouette Score: {silhouette:.3f}\")\n",
    "print(f\"      Categorias agrupadas: {len(df_category_agg)}\")\n",
    "print(f\"   {'─' * 76}\")\n",
    "print(f\"\\n    Experimento mlflow: {experiment_path}\")\n",
    "print(f\"    Artefatos salvos: 8 imagens + 1 CSV\")\n",
    "print(f\"   {'─' * 76}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" \" * 20 + \" Todos os modelos ok e treinados\")\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ml_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
