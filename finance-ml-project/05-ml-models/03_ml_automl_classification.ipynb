{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1413efc4-f756-4712-ae24-0403be97f756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# AUTOML MANUAL COM SCIKIT-LEARN\n",
    "# ============================================\n",
    "\n",
    "import sys\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AUTOML COM SCIKIT-LEARN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 1: CARREGAR E PREPARAR DADOS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 1: CARREGAMENTO E PREPARACAO DOS DADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "SOURCE_TABLE = \"finance_silver.transacoes_silver\"\n",
    "print(f\"\\nCarregando: {SOURCE_TABLE}\")\n",
    "\n",
    "df_silver = spark.read.table(SOURCE_TABLE)\n",
    "\n",
    "df_pandas = df_silver.select(\n",
    "    \"categoria\",\n",
    "    \"valor\",\n",
    "    \"tipo\",\n",
    "    \"dia_semana\",\n",
    "    \"mes\",\n",
    "    \"trimestre\",\n",
    "    \"alto_valor\"\n",
    ").filter(\n",
    "    F.col(\"categoria\").isNotNull()\n",
    ").toPandas()\n",
    "\n",
    "print(f\"\\nDados carregados: {len(df_pandas):,} linhas\")\n",
    "print(f\"Features: {len(df_pandas.columns) - 1}\")\n",
    "print(f\"Target: categoria ({df_pandas['categoria'].nunique()} classes)\")\n",
    "\n",
    "print(\"\\nDistribuicao de Classes:\")\n",
    "print(df_pandas['categoria'].value_counts())\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "display(df_pandas.head())\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 2: FEATURES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 2: PREPARACAO DE FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_pandas['tipo_encoded'] = le.fit_transform(df_pandas['tipo'])\n",
    "\n",
    "features = ['valor', 'tipo_encoded', 'dia_semana', 'mes', 'trimestre', 'alto_valor']\n",
    "X = df_pandas[features]\n",
    "y = df_pandas['categoria']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nDados divididos:\")\n",
    "print(f\"Treino: {len(X_train):,} amostras\")\n",
    "print(f\"Teste: {len(X_test):,} amostras\")\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 3: COMPARAR MULTIPLOS MODELOS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 3: COMPARANDO MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTestando 8 algoritmos\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_jobs=-1),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Treinando {name}\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1-Score': f1,\n",
    "        'CV F1 Mean': cv_mean,\n",
    "        'CV F1 Std': cv_std,\n",
    "        'model_object': model\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f} | F1: {f1:.4f} | CV F1: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANKING DOS MODELOS\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df[['Model', 'Accuracy', 'F1-Score', 'CV F1 Mean']].to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 4: TUNAR MELHOR MODELO\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 4: TUNANDO MELHOR MODELO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\nMelhor modelo: {best_model_name}\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight': ['balanced', None]\n",
    "    }\n",
    "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "    base_model = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'penalty': ['l2'],\n",
    "        'class_weight': ['balanced', None]\n",
    "    }\n",
    "    base_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "    \n",
    "else:\n",
    "    print(\"Usando melhor modelo sem tuning adicional\")\n",
    "    best_model = results_df.iloc[0]['model_object']\n",
    "    param_grid = None\n",
    "\n",
    "if param_grid:\n",
    "    print(f\"\\nTunando hiperparametros com GridSearchCV\")\n",
    "    print(\"Isso pode levar varios minutos\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        base_model,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nMelhores parametros: {grid_search.best_params_}\")\n",
    "    print(f\"Melhor F1-Score (CV): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 5: AVALIAR MODELO FINAL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 5: AVALIACAO DO MODELO FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "\n",
    "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "f1_final = f1_score(y_test, y_pred_final, average='weighted')\n",
    "\n",
    "print(f\"\\nMetricas no conjunto de teste:\")\n",
    "print(f\"Accuracy: {accuracy_final:.4f}\")\n",
    "print(f\"F1-Score: {f1_final:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=sorted(y.unique()),\n",
    "            yticklabels=sorted(y.unique()))\n",
    "plt.title(f'Matriz de Confusao - {best_model_name}')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Previsto')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/automl_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 6: FEATURE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 6: IMPORTANCIA DAS FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(feature_importance_df.to_string(index=False))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'])\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/automl_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nFeature importance nao disponivel para este modelo\")\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 7: SALVAR MODELO NO MLFLOW\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 7: SALVANDO MODELO NO MLFLOW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mlflow.set_experiment(\"/Users/andre.bomfim99@gmail.com/finance-ml-experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"AutoML_ScikitLearn\") as run:\n",
    "    \n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    mlflow.log_param(\"n_samples\", len(df_pandas))\n",
    "    mlflow.log_param(\"n_features\", len(features))\n",
    "    mlflow.log_param(\"n_classes\", y.nunique())\n",
    "    \n",
    "    if param_grid and hasattr(grid_search, 'best_params_'):\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            mlflow.log_param(f\"tuned_{param}\", value)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy_final)\n",
    "    mlflow.log_metric(\"f1_score\", f1_final)\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        best_model, \n",
    "        \"automl_model\",\n",
    "        input_example=X_test.head(1)\n",
    "    )\n",
    "    \n",
    "    mlflow.log_artifact('/tmp/automl_confusion_matrix.png')\n",
    "    \n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        mlflow.log_artifact('/tmp/automl_feature_importance.png')\n",
    "    \n",
    "    print(f\"\\nModelo salvo no MLflow\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "\n",
    "# ============================================\n",
    "# ETAPA 8: COMO USAR O MODELO\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ETAPA 8: COMO USAR O MODELO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nExemplo de uso:\")\n",
    "print(f\"\"\"\n",
    "import mlflow\n",
    "\n",
    "# Carregar modelo\n",
    "model_uri = \"runs:/{run.info.run_id}/automl_model\"\n",
    "modelo = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# Preparar novos dados\n",
    "import pandas as pd\n",
    "\n",
    "df_new = pd.DataFrame({{\n",
    "    'valor': [1000, 5000],\n",
    "    'tipo_encoded': [1, 0],\n",
    "    'dia_semana': [1, 5],\n",
    "    'mes': [6, 12],\n",
    "    'trimestre': [2, 4],\n",
    "    'alto_valor': [0, 1]\n",
    "}})\n",
    "\n",
    "# Fazer predicoes\n",
    "predicoes = modelo.predict(df_new)\n",
    "print(predicoes)\n",
    "\"\"\")\n",
    "\n",
    "# ============================================\n",
    "# RESUMO FINAL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AUTOML CONCLUIDO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nResumo Executivo:\")\n",
    "print(f\"Dataset: {len(df_pandas):,} transacoes\")\n",
    "print(f\"Classes: {y.nunique()}\")\n",
    "print(f\"Modelos testados: {len(models)}\")\n",
    "print(f\"Melhor modelo: {best_model_name}\")\n",
    "print(f\"Accuracy: {accuracy_final:.2%}\")\n",
    "print(f\"F1-Score: {f1_final:.2%}\")\n",
    "print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\nProximos passos:\")\n",
    "print(\"1. Analisar feature importance\")\n",
    "print(\"2. Testar modelo em novos dados\")\n",
    "print(\"3. Registrar no Model Registry\")\n",
    "print(\"4. Monitorar performance em producao\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_ml_automl_classification",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
