{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b93d011-59e4-4422-8db2-687cc67938c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSTALACAO DO FEATURE STORE\n",
    "# ============================================\n",
    "\n",
    "print(\"verificando feature store\")\n",
    "\n",
    "try:\n",
    "    from databricks.feature_store import FeatureStoreClient\n",
    "    print(\"feature store ja instalado\")\n",
    "except ImportError:\n",
    "    print(\"\\ninstalando databricks-feature-store\")\n",
    "    \n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    result = subprocess.run([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \n",
    "        \"databricks-feature-store\"\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nfeature store instalado\")\n",
    "        print(\"reiniciando python\")\n",
    "        dbutils.library.restartPython()\n",
    "    else:\n",
    "        print(\"\\nerro na instalacao\")\n",
    "        print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "176053c9-c3ac-4e1f-adef-5517b4e1e5a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATABRICKS FEATURE STORE\n",
    "# ============================================\n",
    "\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "print(\"feature store client inicializado\")\n",
    "print(\"objetivo: criar tabela de features reutilizavel\")\n",
    "\n",
    "# ============================================\n",
    "# CARREGAR DADOS\n",
    "# ============================================\n",
    "\n",
    "df_silver = spark.read.table(\"finance_silver.transacoes_silver\")\n",
    "\n",
    "# ============================================\n",
    "# CRIAR FEATURES AGREGADAS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\ncriando features agregadas por categoria\")\n",
    "\n",
    "df_features_categoria = df_silver.groupBy(\"categoria\").agg(\n",
    "    F.avg(\"valor\").alias(\"valor_medio\"),\n",
    "    F.stddev(\"valor\").alias(\"valor_desvio_padrao\"),\n",
    "    F.min(\"valor\").alias(\"valor_minimo\"),\n",
    "    F.max(\"valor\").alias(\"valor_maximo\"),\n",
    "    F.count(\"*\").alias(\"total_transacoes\"),\n",
    "    F.countDistinct(\"ano_mes\").alias(\"meses_ativos\"),\n",
    "    F.sum(F.when(F.col(\"alto_valor\") == True, 1).otherwise(0)).alias(\"qtd_alto_valor\"),\n",
    "    F.mode(\"dia_semana\").alias(\"dia_semana_mais_comum\")\n",
    ").withColumn(\n",
    "    \"pct_alto_valor\",\n",
    "    F.round((F.col(\"qtd_alto_valor\") / F.col(\"total_transacoes\")) * 100, 2)\n",
    ")\n",
    "\n",
    "print(f\"\\nfeatures criadas: {len(df_features_categoria.columns)} colunas\")\n",
    "print(f\"categorias: {df_features_categoria.count()}\")\n",
    "\n",
    "print(\"\\namostra das features:\")\n",
    "display(df_features_categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd695ac6-c36e-480a-9f3a-665a492b4566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# REGISTRAR FEATURES NO FEATURE STORE\n",
    "# ============================================\n",
    "\n",
    "print(\"registrando features no feature store\")\n",
    "\n",
    "feature_table_name = \"finance_ml_project.categoria_features\"\n",
    "\n",
    "try:\n",
    "    fs.create_table(\n",
    "        name=feature_table_name,\n",
    "        primary_keys=[\"categoria\"],\n",
    "        df=df_features_categoria,\n",
    "        description=\"Features agregadas por categoria para classificacao de transacoes\"\n",
    "    )\n",
    "    print(f\"\\ntabela criada: {feature_table_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"\\ntabela ja existe, atualizando\")\n",
    "        fs.write_table(\n",
    "            name=feature_table_name,\n",
    "            df=df_features_categoria,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "        print(f\"tabela atualizada: {feature_table_name}\")\n",
    "    else:\n",
    "        print(f\"\\nerro: {e}\")\n",
    "\n",
    "# ============================================\n",
    "# VERIFICAR CRIACAO\n",
    "# ============================================\n",
    "\n",
    "print(\"\\ninformacoes da tabela:\")\n",
    "print(f\"nome: {feature_table_name}\")\n",
    "print(f\"primary key: categoria\")\n",
    "print(f\"features: {len(df_features_categoria.columns) - 1}\")\n",
    "\n",
    "print(\"\\nuso das features:\")\n",
    "print(\"estas features podem ser usadas em:\")\n",
    "print(\"- modelos de ml (join automatico com feature store)\")\n",
    "print(\"- analises ad-hoc\")\n",
    "print(\"- dashboards\")\n",
    "print(\"- outras aplicacoes\")\n",
    "\n",
    "print(\"\\nfeature store concluido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdef0eb-52b9-43fc-9f7d-8dfed4ea0058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATABRICKS FEATURE STORE\n",
    "# ============================================\n",
    "\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "print(\"feature store client inicializado\")\n",
    "\n",
    "# ============================================\n",
    "# CARREGAR DADOS\n",
    "# ============================================\n",
    "\n",
    "df_silver = spark.read.table(\"finance_silver.transacoes_silver\")\n",
    "\n",
    "# ============================================\n",
    "# CRIAR FEATURES AGREGADAS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\ncriando features agregadas por categoria\")\n",
    "\n",
    "df_features_categoria = df_silver.groupBy(\"categoria\").agg(\n",
    "    F.avg(\"valor\").alias(\"valor_medio\"),\n",
    "    F.stddev(\"valor\").alias(\"valor_desvio_padrao\"),\n",
    "    F.min(\"valor\").alias(\"valor_minimo\"),\n",
    "    F.max(\"valor\").alias(\"valor_maximo\"),\n",
    "    F.count(\"*\").alias(\"total_transacoes\"),\n",
    "    F.countDistinct(\"ano_mes\").alias(\"meses_ativos\"),\n",
    "    F.sum(F.when(F.col(\"alto_valor\") == True, 1).otherwise(0)).alias(\"qtd_alto_valor\"),\n",
    "    F.mode(\"dia_semana\").alias(\"dia_semana_mais_comum\")\n",
    ").withColumn(\n",
    "    \"pct_alto_valor\",\n",
    "    F.round((F.col(\"qtd_alto_valor\") / F.col(\"total_transacoes\")) * 100, 2)\n",
    ")\n",
    "\n",
    "print(f\"\\nfeatures criadas: {len(df_features_categoria.columns)} colunas\")\n",
    "print(f\"categorias: {df_features_categoria.count()}\")\n",
    "\n",
    "print(\"\\namostra das features:\")\n",
    "display(df_features_categoria)\n",
    "\n",
    "# ============================================\n",
    "# VERIFICAR E CRIAR SCHEMA SE NECESSARIO\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nverificando schema\")\n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"finance_gold\"\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    print(f\"schema {catalog_name}.{schema_name} disponivel\")\n",
    "except Exception as e:\n",
    "    print(f\"aviso: {e}\")\n",
    "    catalog_name = \"main\"\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "    print(f\"usando catalog {catalog_name}\")\n",
    "\n",
    "# ============================================\n",
    "# REGISTRAR FEATURES NO FEATURE STORE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nregistrando features no feature store\")\n",
    "\n",
    "feature_table_name = f\"{catalog_name}.{schema_name}.categoria_features\"\n",
    "\n",
    "try:\n",
    "    fs.create_table(\n",
    "        name=feature_table_name,\n",
    "        primary_keys=[\"categoria\"],\n",
    "        df=df_features_categoria,\n",
    "        description=\"features agregadas por categoria para classificacao de transacoes\"\n",
    "    )\n",
    "    print(f\"\\ntabela criada: {feature_table_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"\\ntabela ja existe, atualizando\")\n",
    "        fs.write_table(\n",
    "            name=feature_table_name,\n",
    "            df=df_features_categoria,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "        print(f\"tabela atualizada: {feature_table_name}\")\n",
    "    else:\n",
    "        print(f\"\\nerro: {e}\")\n",
    "        print(\"\\ntentando abordagem alternativa\")\n",
    "        \n",
    "        df_features_categoria.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(feature_table_name)\n",
    "        \n",
    "        print(f\"tabela criada via delta: {feature_table_name}\")\n",
    "\n",
    "# ============================================\n",
    "# VERIFICAR CRIACAO\n",
    "# ============================================\n",
    "\n",
    "print(\"\\ninformacoes da tabela:\")\n",
    "print(f\"nome: {feature_table_name}\")\n",
    "print(f\"primary key: categoria\")\n",
    "print(f\"features: {len(df_features_categoria.columns) - 1}\")\n",
    "\n",
    "print(\"\\nuso das features:\")\n",
    "print(\"estas features podem ser usadas em:\")\n",
    "print(\"- modelos de ml\")\n",
    "print(\"- analises ad-hoc\")\n",
    "print(\"- dashboards\")\n",
    "\n",
    "print(\"\\ncomo usar:\")\n",
    "print(f\"\"\"\n",
    "df_features = spark.read.table('{feature_table_name}')\n",
    "\n",
    "df_train = spark.read.table('finance_silver.transacoes_silver')\n",
    "df_with_features = df_train.join(df_features, on='categoria', how='left')\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nfeature store concluido\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_ml_feature_store",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
